{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68f670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import dlib\n",
    "import sys\n",
    "import argparse\n",
    "from training import *\n",
    "from pickesave import *\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752cd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol2cart(rho, phi): #Convert polar coordinates to cartesian coordinates for computation of optical strain\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d77739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeStrain(u, v):\n",
    "    u_x= u - pd.DataFrame(u).shift(-1, axis=1)\n",
    "    v_y= v - pd.DataFrame(v).shift(-1, axis=0)\n",
    "    u_y= u - pd.DataFrame(u).shift(-1, axis=0)\n",
    "    v_x= v - pd.DataFrame(v).shift(-1, axis=1)\n",
    "    os = np.array(np.sqrt(u_x**2 + v_y**2 + 1/2 * (u_y+v_x)**2).ffill().ffill())\n",
    "    return os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0b3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_top(images, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2,2), selected_frames=3, eps=1e-7):\n",
    "    video_length = len(images)\n",
    "    middle_frame = video_length // 2\n",
    "    selected_3frames = [0, middle_frame, video_length - 1]\n",
    "    \n",
    "    img_seq = []\n",
    "    for i in selected_3frames:\n",
    "        img = cv2.resize(images[i], (128, 64))\n",
    "        \n",
    "        # Check if the image is grayscale, and expand dimensions to add a channel\n",
    "        ##this was not in sir's code\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=-1)  # Add a dummy channel for grayscale images\n",
    "        \n",
    "        img_seq.append(img)\n",
    "\n",
    "    # Convert list to NumPy array\n",
    "    img_seq = np.array(img_seq)\n",
    "\n",
    "    # Ensure img_seq is 4D (frames, height, width, channels)\n",
    "    if img_seq.ndim == 3:\n",
    "        img_seq = np.expand_dims(img_seq, axis=-1)  # Add a channel axis if missing\n",
    "\n",
    "    # Transpose the img_seq to extract features from XT and YT planes\n",
    "    xt = np.transpose(img_seq, (1, 0, 2, 3))\n",
    "    yt = np.transpose(img_seq, (2, 0, 1, 3))\n",
    "\n",
    "    # Continue with HOG extraction...\n",
    "    hist_xy_plane = np.empty(0, dtype=np.float32)\n",
    "    hist_xt_plane = np.empty(0, dtype=np.float32)\n",
    "    hist_yt_plane = np.empty(0, dtype=np.float32)\n",
    "\n",
    "    n_frames_xy_plane = img_seq.shape[0]\n",
    "    n_frames_xt_plane = xt.shape[0]\n",
    "    n_frames_yt_plane = yt.shape[0]\n",
    "\n",
    "    for i in range(n_frames_xy_plane):\n",
    "        hist_xy, hog_xy = hog(img_seq[i], orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                              cells_per_block=cells_per_block, block_norm='L2', visualize=True, channel_axis=-1)\n",
    "        hist_xy_plane = np.concatenate((hist_xy_plane, hist_xy))\n",
    "\n",
    "    for i in range(n_frames_xt_plane):\n",
    "        hist_xt, hog_xt = hog(xt[i], orientations=orientations, pixels_per_cell=(3, 8),\n",
    "                              cells_per_block=(2, 2), block_norm='L2', visualize=True, channel_axis=-1)\n",
    "        hist_xt_plane = np.concatenate((hist_xt_plane, hist_xt))\n",
    "\n",
    "    for i in range(n_frames_yt_plane):\n",
    "        hist_yt, hog_yt = hog(yt[i], orientations=orientations, pixels_per_cell=(3, 8),\n",
    "                              cells_per_block=cells_per_block, block_norm='L2', visualize=True, channel_axis=-1)\n",
    "        hist_yt_plane = np.concatenate((hist_yt_plane, hist_yt))\n",
    "\n",
    "    hog_top = np.concatenate((hist_xy_plane, hist_xt_plane, hist_yt_plane))\n",
    "    hog_top = hog_top / np.linalg.norm(hog_top + eps)\n",
    "\n",
    "    return hog_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preprocess(final_images, k):\n",
    "    #Path to the pre-trained dlib shape predictor model.\n",
    "    predictor_model = \"Utils\\\\shape_predictor_68_face_landmarks.dat\"\n",
    "    #Initialized dlib's face detector.\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    # Initialized dlib's shape predictor for facial landmarks.\n",
    "    face_pose_predictor = dlib.shape_predictor(predictor_model)\n",
    "    #An empty list to store the processed data for all videos.\n",
    "    dataset_hogof = []\n",
    "    for video in range(len(final_images)):\n",
    "      OFF_video = []\n",
    "      for img_count in range(final_images[video].shape[0]-k):\n",
    "        #Taking only images separted by k interval frames\n",
    "        img1 = final_images[video][img_count]\n",
    "        img2 = final_images[video][img_count+k]\n",
    "        #Detecting Facial Landmarks, For the first frame, it attempts to detect a face. \n",
    "        #If no face is detected, it continues to the next frame until a face is found.\n",
    "        #Once a face is detected, it uses face_pose_predictor to get the landmarks.\n",
    "        \n",
    "        if (img_count==0):\n",
    "            reference_img = img1\n",
    "            detect = face_detector(reference_img,1)\n",
    "            next_img=0 #Loop through the frames until all the landmark is detected\n",
    "            while (len(detect)==0):\n",
    "                next_img+=1\n",
    "                reference_img = final_images[video][img_count+next_img]\n",
    "                detect = face_detector(reference_img,1)\n",
    "            shape = face_pose_predictor(reference_img,detect[0])\n",
    "            \n",
    "            #Left Eye\n",
    "            x11=max(shape.part(36).x - 15, 0)\n",
    "            y11=shape.part(36).y \n",
    "            x12=shape.part(37).x \n",
    "            y12=max(shape.part(37).y - 15, 0)\n",
    "            x13=shape.part(38).x \n",
    "            y13=max(shape.part(38).y - 15, 0)\n",
    "            x14=min(shape.part(39).x + 15, 128)\n",
    "            y14=shape.part(39).y \n",
    "            x15=shape.part(40).x \n",
    "            y15=min(shape.part(40).y + 15, 128)\n",
    "            x16=shape.part(41).x \n",
    "            y16=min(shape.part(41).y + 15, 128)\n",
    "            \n",
    "            #Right Eye\n",
    "            x21=max(shape.part(42).x - 15, 0)\n",
    "            y21=shape.part(42).y \n",
    "            x22=shape.part(43).x \n",
    "            y22=max(shape.part(43).y - 15, 0)\n",
    "            x23=shape.part(44).x \n",
    "            y23=max(shape.part(44).y - 15, 0)\n",
    "            x24=min(shape.part(45).x + 15, 128)\n",
    "            y24=shape.part(45).y \n",
    "            x25=shape.part(46).x \n",
    "            y25=min(shape.part(46).y + 15, 128)\n",
    "            x26=shape.part(47).x \n",
    "            y26=min(shape.part(47).y + 15, 128)\n",
    "            \n",
    "            #ROI 1 (Left Eyebrow)\n",
    "            x31=max(shape.part(17).x - 12, 0)\n",
    "            y32=max(shape.part(19).y - 12, 0)\n",
    "            x33=min(shape.part(21).x + 12, 128)\n",
    "            y34=min(shape.part(41).y + 12, 128)\n",
    "            \n",
    "            #ROI 2 (Right Eyebrow)\n",
    "            x41=max(shape.part(22).x - 12, 0)\n",
    "            y42=max(shape.part(24).y - 12, 0)\n",
    "            x43=min(shape.part(26).x + 12, 128)\n",
    "            y44=min(shape.part(46).y + 12, 128)\n",
    "            \n",
    "            #ROI 3 #Mouth\n",
    "            x51=max(shape.part(60).x - 12, 0)\n",
    "            y52=max(shape.part(50).y - 12, 0)\n",
    "            x53=min(shape.part(64).x + 12, 128)\n",
    "            y54=min(shape.part(57).y + 12, 128)\n",
    "            \n",
    "            #Nose landmark\n",
    "            x61=shape.part(28).x\n",
    "            y61=shape.part(28).y\n",
    "    \n",
    "        #Compute Optical Flow Features\n",
    "        #optical_flow = cv2.DualTVL1OpticalFlow_create() #Depends on cv2 version\n",
    "        optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        #3D array ouput of the optical flow, each element contains a (u,v) array where u means horizontal displacement at position\n",
    "        #(x,y) and v means vertical displacement at position (x,y)\n",
    "        flow = optical_flow.calc(img1, img2, None)\n",
    "        \n",
    "        #This converts the Cartesian coordinates (u, v) of the flow vectors to polar coordinates (magnitude, angle).\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1]) \n",
    "        #This converts the polar coordinates back to Cartesian coordinates. This step is based on the assumption that pol2cart is defined elsewhere.\n",
    "        u, v = pol2cart(magnitude, angle)\n",
    "        \n",
    "        #This computes the strain (a measure of deformation) from the displacement vectors (u, v). The specifics of computeStrain \n",
    "        #depend on its implementation, but it typically quantifies how much the shape of the object has changed between img1 and img2.\n",
    "        os = computeStrain(u, v)\n",
    "                \n",
    "        #Features Concatenation into 128x128x3\n",
    "        final = np.zeros((128, 128, 3))\n",
    "        final[:,:,0] = u\n",
    "        final[:,:,1] = v\n",
    "        final[:,:,2] = os\n",
    "        \n",
    "        #Remove global head movement by minus nose region\n",
    "        final[:, :, 0] = abs(final[:, :, 0] - final[y61-5:y61+6, x61-5:x61+6, 0].mean())\n",
    "        final[:, :, 1] = abs(final[:, :, 1] - final[y61-5:y61+6, x61-5:x61+6, 1].mean())\n",
    "        final[:, :, 2] = final[:, :, 2] - final[y61-5:y61+6, x61-5:x61+6, 2].mean()\n",
    "        \n",
    "        #Eye masking\n",
    "        left_eye = [(x11, y11), (x12, y12), (x13, y13), (x14, y14), (x15, y15), (x16, y16)]\n",
    "        right_eye = [(x21, y21), (x22, y22), (x23, y23), (x24, y24), (x25, y25), (x26, y26)]\n",
    "        cv2.fillPoly(final, [np.array(left_eye)], 0)\n",
    "        cv2.fillPoly(final, [np.array(right_eye)], 0)\n",
    "        \n",
    "        #Extracts the regions of interest (eyebrows and mouth) and resamples them to a fixed size \n",
    "        #Concatenates the resampled regions into a final image (42x42x3).\n",
    "        #ROI Selection -> Image resampling into 42x22x3\n",
    "        final_image = np.zeros((42, 42, 3))\n",
    "        final_image[:21, :, :] = cv2.resize(final[min(y32, y42) : max(y34, y44), x31:x43, :], (42, 21))\n",
    "        final_image[21:42, :, :] = cv2.resize(final[y52:y54, x51:x53, :], (42, 21))\n",
    "         # HOG-TOP Feature Extraction for the full video\n",
    "        hog_top_features = extract_hog_top(final_images[video])\n",
    "\n",
    "            # Append optical flow features and HOG-TOP features as a tuple\n",
    "        OFF_video.append([final_image, hog_top_features])\n",
    "        \n",
    "      dataset_hogof.append(OFF_video)\n",
    "      print('Video', video, 'Done')\n",
    "    print('All Done')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048931a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18f57ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  D:\\SOFTNet-first\\variables\\final_images  has been loaded successfully.\n",
      "The  D:\\SOFTNet-first\\variables\\k  has been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "final_images=readvar('D:\\\\SOFTNet-first\\\\variables\\\\final_images')\n",
    "k=readvar('D:\\\\SOFTNet-first\\\\variables\\\\k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9973232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------ Feature Extraction & Pre-processing ------\n",
      "Video 0 Done\n",
      "Video 1 Done\n",
      "Video 2 Done\n",
      "Video 3 Done\n",
      "Video 4 Done\n",
      "Video 5 Done\n",
      "Video 6 Done\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Feature Extraction & Pre-processing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ------ Feature Extraction & Pre-processing ------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m dataset_hogof \u001b[38;5;241m=\u001b[39m extract_preprocess(final_images, k)\n",
      "Cell \u001b[1;32mIn[12], line 68\u001b[0m, in \u001b[0;36mextract_preprocess\u001b[1;34m(final_images, k)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Region of Interest (ROI) Selection and resizing\u001b[39;00m\n\u001b[0;32m     67\u001b[0m final_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 68\u001b[0m final_image[:\u001b[38;5;241m21\u001b[39m, :, :] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(final[\u001b[38;5;28mmin\u001b[39m(shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m19\u001b[39m)\u001b[38;5;241m.\u001b[39my, shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m24\u001b[39m)\u001b[38;5;241m.\u001b[39my):\u001b[38;5;28mmax\u001b[39m(shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m41\u001b[39m)\u001b[38;5;241m.\u001b[39my, shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m46\u001b[39m)\u001b[38;5;241m.\u001b[39my),\n\u001b[0;32m     69\u001b[0m                                      shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m17\u001b[39m)\u001b[38;5;241m.\u001b[39mx:shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m26\u001b[39m)\u001b[38;5;241m.\u001b[39mx, :], (\u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m21\u001b[39m))\n\u001b[0;32m     70\u001b[0m final_image[\u001b[38;5;241m21\u001b[39m:\u001b[38;5;241m42\u001b[39m, :, :] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(final[shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39my:shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m57\u001b[39m)\u001b[38;5;241m.\u001b[39my, shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m60\u001b[39m)\u001b[38;5;241m.\u001b[39mx:shape\u001b[38;5;241m.\u001b[39mpart(\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mx, :], (\u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m21\u001b[39m))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# HOG-TOP Feature Extraction for the full video\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction & Pre-processing\n",
    "print('\\n ------ Feature Extraction & Pre-processing ------')\n",
    "dataset_hogof = extract_preprocess(final_images, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save value once in file\n",
    "savevar('dataset_hogof',dataset_hogof)\n",
    "\n",
    "#read values from file\n",
    "dataset=readvar('dataset_hogof')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOFTNet_environment",
   "language": "python",
   "name": "softnet_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
